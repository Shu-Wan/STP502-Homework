
\documentclass[11pt]{article}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{color}
\usepackage[margin=1in,nohead]{geometry}
\usepackage[mathscr]{euscript}
\usepackage{enumitem}
\usepackage{url}
\usepackage{listings}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{hyperref}

\newcommand{\mc}{\mathcal}
\newcommand{\mbf}{\mathbf}
\newcommand{\mb}{\mathbb}
\newcommand{\msc}{\mathscr}
\newcommand{\goesto}{\rightarrow}
\newcommand{\note}{{\bf Note: }}
\newcommand{\vspan}{\text{span}}

\newcommand{\R}{\mb{R}}
\newcommand{\nat}{\mb{N}}

\newcommand{\A}{\mathbf{A}}
\newcommand{\B}{\mathbf{B}}
\newcommand{\C}{\mathbf{C}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\z}{\mathbf{z}}
\renewcommand{\b}{\mathbf{b}}
\renewcommand{\u}{\mathbf{u}}
\renewcommand{\v}{\mathbf{v}}
\newcommand{\ones}{\mathbf{1}}
\newcommand{\zero}{\mathbf{0}}

\newcommand{\Eqn}[1]{\begin{align*} #1 \end{align*}}
\newcommand{\bbm}{\begin{bmatrix}}
\newcommand{\ebm}{\end{bmatrix}}
\newcommand{\bpm}{\begin{pmatrix}}
\newcommand{\epm}{\end{pmatrix}}

\newcommand{\Sol}{\par {\bf Solution:}}
\newcommand{\sample}[1]{#1_1 , \dots , #1_n}
\newcommand{\order}[1]{X_{(#1)}}
\newcommand{\Partial}[1]{\frac{\partial}{\partial #1}}
\newcommand{\SecPartial}[1]{\frac{\partial^2}{\partial {#1}^2}}

\DeclareMathOperator*{\argmin}{\arg\min}
\DeclareMathOperator*{\argmax}{\arg\max}

\setlength{\parskip}{6pt}
\setlength{\parindent}{0pt}
\allowdisplaybreaks[4]
\lstset{
  basicstyle=\ttfamily,
  columns=fullflexible,
  frame=single,
  breaklines=true,
  postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space},
}

\begin{document}

\begin{center}
\Large{
\textbf{STP 502, Spring 2023, Homework 4} \\
Due: Monday, Feb 27, 2023. \\
Shu Wan (1226038322)
}
\end{center}
\subsection*{7.44}
Let $\sample{X}$ be iid $n(\theta, 1)$. Show that the best unbiased estimator of $\theta^2$ is $\bar X^2 - (1/ n)$.
Calculate its variance (use Stein's Identity from Section 3.6), and show that it is greater than the Cram\'{e}r-Rao Lower Bound.

\Sol

We all know that $\bar X$ is a complete and sufficient statistic for $n(\theta, 1)$. $Y = \bar X^2 - \frac{1}{n}$ is a function of $\bar X$ and 
\[
E(Y) = E(\bar X^2 - \frac{1}{n}) =  Var(\bar X) + E(\bar X)^2 - \frac{1}{n} = \frac{1}{n} + \theta^2 - \frac{1}{n} = \theta^2.
\]
By Theorem 7.3.23, $Y$ is the UMVUE of $\theta^2$.

The variance of $Y$ is 
\[
Var(Y) = Var(\bar X^2) = E(\bar X^4) - [E(\bar X^2)]^4, \quad Y \sim n(\theta, \frac{1}{n}),
\]

where
\begin{align*}
    E(\bar X^4) &= E(\bar X^3(\bar X - \theta + \theta)) \\
    &= E[\bar X^3 (\bar X - \theta)] + \theta E(\bar X^3) \\
    &= \frac{1}{n}E(3 \bar X^2) + \theta E(\bar X^3) && \text{(Stein's Identity)}
\end{align*}
and
\begin{align*}
E(\bar X^3) &= E[\bar X^2(\bar X - \theta + \theta)] \\
&= E[\bar X^2 (\bar X - \theta)] + \theta E\bar X^2 \\
&= 2\frac{1}{n}E(\bar X) + \theta E\bar X^2 && \text{(Stein's Identity)} \\
&= \frac{3\theta}{n} + \theta^3,
\end{align*}
and
\[
E(\bar X^2) = \frac{1}{n} + \theta^2.
\]
Plug in these values to the variance equation,
\[
\Longrightarrow Var(Y) =  \frac{4\theta^2}{n} + \frac{2}{n^2} > \frac{4\theta^2}{n}.
\]

The Cram\'{e}r Rao Lower Bound for the estimator $\tau(\x)$ of$\theta^2$ is given by
\[
\frac{E [\Partial E_\theta \tau(\x)]}{-nE_\theta [\SecPartial{\theta} \log f(x|\theta)]},
\]
where
\begin{align*}
    E_\theta [\SecPartial{\theta} \log f(x|\theta)] &= E_\theta [\SecPartial{\theta} (-\frac{1}{2} \log 2\pi - \frac{1}{2}(x-\theta)^2] \\
    &= E_\theta[\Partial \theta (x-\theta)] = -1.
\end{align*}

Therefore, the CRLB is $\frac{4\theta^2}{n}$, which small than the $Y$.

\subsection*{7.46}
Let $X_1$, $X_2$, and $X_3$ be a random sample of size three from a uniform$(\theta, 2\theta)$ distribution, where $\theta > 0$.
\begin{enumerate}[label=(\alph*)]
    \item Find the method of moments estimator of $\theta$.
    \item Find the MLE, $\hat \theta$, and find a constant $k$ such that $E_\theta(k\hat \theta) = \theta$.
    \item Which of the two estimators can be improved by using sufficiency? How?
    \item Find the method of moments estimate and the MLE of $\theta$ based on the data
    \[
    1.29, .86, 1.33,
    \]
    three observations of average berry sizes (in centimeters) of wine grapes.
\end{enumerate}

\Sol
\begin{enumerate}[label=(\alph*)]
    \item $E(X) = \frac{3}{2}\theta = \bar X$. Hence, the method of moments estimator is $\tilde \theta = \frac{2\bar X}{3}$
    \item 
    The likelihood function of $X$ is
    \[
    L(\theta|\X) = \prod X_i = \prod \frac{1}{\theta} I(X_i \in (\theta, 2\theta)) = \frac{1}{\theta^n}I_{(X_{(n)}/2, X_{(1)})}{(\theta)}.
    \]
    This function is a monotone decreasing function. Thus, maximal is achieved at $\hat \theta = X_{(2)}/2$.
    
    Since $E(X_{(n)}) = \frac{2n+1}{n+1}\theta$, we have $E\hat \theta = \frac{2n+1}{2n + 2}\theta$. Let $k = \frac{2n+2}{2n+1} \Longrightarrow E(k\hat \theta) = \theta.$
    \item
    We know that $T(\X) = (X_{(1)}, X_{(n)})$ is a sufficient statistic for $\theta$. By Rao-Blackwell Theorem, $\phi(T) = E(\tau(\theta)|T)$ is an unbiased estimator, if $\tau(\theta)$ is an unbiased estimator and $\phi(T)$ is a better unbiased estimator. However, MLE estimator $\hat \theta$ is a function of $T(\X)$, while method of moments estiamtor is not. Hence, there's no improvement for MLE estimator $\hat \theta$, because $E(\hat \theta)|T) = \hat \theta$, while we could see improvements on method of moments estimator.
    \item
    $\tilde \theta = 2*1.16/3 \approx 0.7733, ~\hat \theta = 0.665$.
\end{enumerate}

\subsection*{7.47}
Suppose that when the radius of a circle is measured, an error is made that has a $n(0, \sigma^2)$ distribution. If $n$ independent measurements are made, find an unbiased estimator of the area of the circle. Is it best unbiased?
\Sol

Let $r$ be the real radius of the circle, then the measured radius $X$ follows $n(r, \sigma^2)$. The real area of a circle is $A = \pi r^2$.
We know that $\bar X$ is a sufficient and  complete statistic for normal distribution, and $\bar X \sim n(r, \sigma^2/n)$.

\[
E(\bar X) = r^2 + \sigma^2/n,
\]
let $\hat A(r) = \pi(\bar X - \sigma^2/n)$, $\hat A(r)$ is an unbiased estimator of circle area $A$. By Theorem 7.3.23, $\hat A(r)$ is the UMVUE of $A$.

\subsection*{7.48}
Suppose that $X_i$, $i = 1, \dots, n$, are iid Bernoulli($p$).
\begin{enumerate}[label=(\alph*)]
    \item Show that the variance of the MLE of $p$ attains the Cram\'{e}r-Rao Lower Bound.
    \item For $n > 4$, show that the product $X_1X_2X_3X_4$ is an unbiased estimator of $p^4$ and use this fact to find the best unbiased estimator of $p^4$.
\end{enumerate}

\Sol
\begin{enumerate}[label=(\alph*)]
    \item
    We know that $\bar X$ is an unbiased estimator for $p$. Furthurmore, we have
    \[
    \Partial p \log L(\theta|\x) = \frac{n}{p}-\frac{n-\sum X_i}{1-p} = \frac{n}{1-p}[\frac{\sum X_i}{n} - (1 + \frac{1}{p(1-p)})].
    \]
    $\bar X$ attains the CRLB.
    \item
    Because all samples are iid,
    \[
    E(X_1X_2X_3X_4) = \prod_1^4E(X_i) = p^4.
    \]
    $Y = X_1X_2X_3X_4$ is an unbiased estimator of $p^4$.
    By Rao-Blackwell Theorem and Theorem 7.3.23, we can construct an estimator based on the sufficient and complete statistic $\sum X_i$,
    \[
    E(Y|\sum X_i = t) = \frac{1 \cdot P(Y = 1, \sum_5^n X_i = t -4)}{P(\sum X_i = t)} = \frac{p^4{n-4 \choose t-4}(1-p)^{n-4}}{{n \choose t} p^t(1-p)^{n-t}} = \frac{{n-4 \choose  t-4}}{{n \choose t}},
    \]
    and $E(Y|\sum X_i = t)$ is the UMVUE of $p^4$.
\end{enumerate}

\subsection*{7.49}
Let $\sample X$ be iid exponential($\lambda$).
\begin{enumerate}[label=(\alph*)]
    \item Find an unbiased estimator of $\lambda$ based only on $y = \min\{\sample X\}$.
    \item Find a better estimator than the one in part(a). Prove that it is better.
    \item The following data are high-stress failure times (in hours) of Kevlar/epoxy spherical vessels used in a sustained pressure environment on the space shuttle:
    \[
    50.1, 70.1, 137.0, 166.9, 170.5, 152.8, 80.5, 123.5, 112.6, 148.5, 160.0, 125.4.
    \]
    Failure times are often modeled with the exponential distribution. Estimate the mean failure time using the estimators from parts (a) and (b).
\end{enumerate}

\Sol
\begin{enumerate}[label=(\alph*)]
    \item
    The minimal order statistic has distribution
    \[
    Y = X_{(1)} \sim f(y) = \frac{n}{\lambda}e^{-ny/\lambda}, \quad Y \sim exp(\lambda/n).
    \]
    $E(Y) = \lambda/n \Longrightarrow E(nY) = \lambda$.
    $nY$ is an unbiased estimator of $\lambda$.
    
    \item
     We know that  $\sum X_i$ is a sufficient and complete statistic for exponential distribution and $E(\sum X_i) = n\lambda$. By Rao-Blackwell Theorem, we can construct an estimator using the sufficient statistic $E(X_{(1)}|\sum X_i) = \frac{\sum X_i}{n}$. By Theorem 7.3.23, this is the UMVUE for $\lambda$.
     
    \item
    $Y = nX_{(1)} = 12*50.1=601.2$, and $\sum X_i / n = 1497.9/12 = 124.825$.
\end{enumerate}

\subsection*{7.50}
Let $\sample X$ be iid $n(\theta, \theta^2), \theta > 0$. For this model both $\bar X$ and $cS$ are unbiased estimators of $\theta$, where $c = \frac{\sqrt{n-1}\Gamma((n-1)/2)}{\sqrt{2}\Gamma(n/2)}$.
\begin{enumerate}[label=(\alph*)]
    \item Prove that for any number $a$ the estimator $a \bar X + (1-a)(cS)$ is an unbiased estimator of $\theta$.
    \item Find the value of $a$ that produces the estimator with minimum variance.
    \item Show that $(\bar X, S^2)$ is a sufficient statistic for $\theta$ but it is not a complete sufficient statistic.
\end{enumerate}

\Sol
\begin{enumerate}[label=(\alph*)]
    \item
    By linearity of expectation, we have,
    \[
    E(a\bar X + (1-a)(cS)) = aE(\bar X) + (1-a)E(cS) = \theta.
    \]
    Hence, $Y = a\bar X + (1-a)(cS)$ is an unbiased estimator of $\theta$.
    
    \item
    \[
    Var(Y) = a^2 Var(\bar X) + (1-a)^2Var(cS),
    \]
    where
    \[
    Var \bar X = \theta^2/n,
    \]
    and 
    \[
    Var(cS) = E(cS)^2 - (E(cS))^2 = c^2E(S^2) - \theta^2 = c^2\theta^2 - \theta^2.
    \].
    Combine them together, we have
    \[
    Var(Y) = (\frac{\theta^2}{n} + (c^2-1)\theta^2)a^2 - a(c-1)\theta^2a + (c-1)\theta^2,
    \]
    is a quadratic function.
    The minimum is at the vertex of the curve, which is at $\frac{c^2-1}{1/n + c^2 - 1}$.

    \emph{Note} this is only true when $\frac{\theta^2}{n} + (c^2-1)\theta^2$ is positive.
    \item
    Using the Factorization Theorem, we can easily show that $(\bar X, S^2)$ is a sufficient statistic for $\theta$, however it's not complete. Let $Y = \bar X - S^2$. We can prove that $E(Y) = 0$, but $Y$ does not equal to 0 everywhere.
\end{enumerate}

\subsection*{7.59}
Let $\sample X$ be iid $n(\mu, \sigma^2)$. Find the best unbiased estimator of $\sigma^p$, where $p$ is a known positive constant, not necessarily an integer.

\Sol

We know that $(\bar X, S^2)$ is a sufficient and complete statistic of $(\mu, \sigma^2)$ and $Y = \frac{(n-1)S^2}{\sigma^2} \sim \chi_{n-1}^2$.
Then
\[
E [Y^{p/2}] = \frac{1}{2^{(n-1)/2}\Gamma(\frac{n-1}{2})} \int t^{\frac{n+p-1}{2}e^{-t/2}}dt.
\]
Notice that the term in the integral is the kernel of $\chi_{n+p-1}$. Therefore, 
\[
E[Y^{p/2}] = 2^{p/2}\frac{\Gamma(\frac{n+p-1}{2})}{\Gamma(\frac{n-1}{2})},
\]
so $(\frac{n-1}{2})^{p/2}S^p\frac{\Gamma(\frac{n-1}{2})}{\Gamma(\frac{n+p-1}{2})}$ is an unbiased estimator of $\sigma^p$. By Theorem 7.3.23, this is also the best unbiased estimator.

\subsection*{7.60}
Let $\sample X$ be iid gamma$(\alpha, \beta)$ with $\alpha$ known. Find the best unbiased estimator of $1/\beta$.

\Sol

When $\alpha$ is known, $Y = \bar X$ is a sufficient and complete statistic of $\beta$ and $Y \sim \text{gamma}(\alpha n, \beta/n)$.
Then,
\[
E[\frac{1}{Y}] = \frac{1}{\Gamma(\alpha n){(\beta/n)}^{\alpha n}} \int y^{\alpha n - 1 - 1}e^{-ny/\beta} dy.
\]
The term in the integral is the kernel of gamma$(\alpha n - 1, \beta/n)$. Hence,
\[
E[\frac{1}{Y}] = E[\frac{1}{\bar X}] = \frac{\Gamma(\alpha n - 1){(\beta/n)}^{\alpha n - 1}}{\Gamma(\alpha n){(\beta/n)}^{\alpha n }} = \frac{n\Gamma(\alpha n - 1)}{\Gamma(\alpha n)} \cdot \frac{1}{\beta} = \frac{1}{\alpha} \cdot \frac{1}{\beta}.
\]
Then $\frac{n\alpha}{\sum X_i}$ is an unbiased estimator of $1/\beta$ and by Theorem 7.3.23, this is the best.

\end{document}